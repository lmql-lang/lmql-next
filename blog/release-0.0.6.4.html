<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LMQL: A query language for programming (large) language models.">
  <meta name="keywords" content="GPT-3, language models, LMQL, programming language, query language, ChatGPT">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"/>
  
  <!-- Primary Meta Tags -->
  <title>Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more</title>
  <meta name="title" content="Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more">
  <meta name="description" content="Among many things, this update contains several bug fixes and improvements. The most notable changes are:">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://lmql.ai/">
  <meta property="og:title" content="Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more">
  <meta property="og:description" content="Among many things, this update contains several bug fixes and improvements. The most notable changes are:">
  <meta property="og:image" content="https://lmql.ai/static/images/lmql-social.png">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://lmql.ai/">
  <meta property="twitter:title" content="Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more">
  <meta property="twitter:description" content="Among many things, this update contains several bug fixes and improvements. The most notable changes are:">
  <meta property="twitter:image" content="https://lmql.ai/static/images/lmql-social.png"">
  
  
  <!--  set relative path to be / -->
  <base href="/">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="./static/css/val.gen.css">
  <link rel="stylesheet" href="./static/css/lmql.css">
  <link rel="stylesheet" href="./static/css/highlight.min.css">
  <script src="https://kit.fontawesome.com/06bb68d804.js" crossorigin="anonymous"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/highlight.min.js"></script>
  <script src="./static/js/lmql.js"></script>
  <script>
    var openPlaygroundElement = null;

function getPlaygroundUrl(next) {
    const host = window.location.host;
    console.log("next is", next)
    if (next) {
      return "https://next.lmql.ai/playground"
    }
    if (host.includes("lmql.ai")) {
        return "https://lmql.ai/playground";
    } else if (host.startsWith("localhost") || host.startsWith("127.0.0.1")) {
        return "http://localhost:3000/playground";
    } else {
        return "https://lbeurerkellner.github.io/green-gold-dachshund-web/playground";
    }
}

function closePlaygroundSnippet() {
    if (openPlaygroundElement) {
        openPlaygroundElement.innerHTML = openPlaygroundElement.originalHTML;
        openPlaygroundElement.classList.remove('playground');
        openPlaygroundElement.style.height = 'auto';

        // show model output div if it exists
        let next = openPlaygroundElement.nextElementSibling;
        while(next.tagName !== 'DIV' && next) {
            next = next.nextElementSibling;
        }

        if (next.classList.contains('highlight-model-output')) {
            next.style.display = 'block';
        }

            openPlaygroundElement = null;
        }
}

function openPlaygroundSnippet(link, snippet) {
    closePlaygroundSnippet();

    const playground = getPlaygroundUrl(snippet.includes("next-"));
    console.log("playground url: " + playground);

    // this is a that was clicked, replace parent div with iframe (temporarily)
    const container = link.parentElement;
    container.classList.add('playground');
    const iframe = document.createElement('iframe');
    iframe.src = ""
    iframe.src = playground + '?embed=' + snippet + ".json"
    iframe.style.width = '100%';
    iframe.style.height = '100%';
    iframe.style.border = 'none';
    
    const height = Math.max(400, container.clientHeight);
    container.originalHTML = container.innerHTML;
    container.innerHTML = '';
    container.style.height = height + 'px';
    container.appendChild(iframe);

    // hide the model output div if it exists
    let next = container.nextElementSibling;
    while(next.tagName !== 'DIV' && next) {
        next = next.nextElementSibling;
    }

    if (next.classList.contains('highlight-model-output')) {
        next.style.display = 'none';
    }

    openPlaygroundElement = container;
}
  </script>

  <!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "f7d2a6b1a0624c51ae4dab9a4239b77d"}'></script><!-- End Cloudflare Web Analytics -->
</head>
<body>
  <div class="topnav">
    <a href="/">
      <img src="./static/images/lmql-text.png" alt="LMQL logo" class="logo">
    </a>
    <a href="https://discord.gg/7eJP4fcyNT" class="hide-on-small">
      üí¨
      Discord
    </a>
    <a href="blog">
      üìù
      Blog
    </a>
    <a href="https://docs.lmql.ai">
      üìñ
      Docs
    </a>
    <a href="https://github.com/eth-sri/lmql">
      üì¶
      GitHub</a>
  </div>


<section class="section blog " id="release-0.0.6.4">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h5 class="title is-2 has-text-left" style="margin-bottom: 2pt;">
            <a href="blog/release-0.0.6.4.html" class="anchor">
                Releasing LMQL 0.0.6.4: LMTP, Azure, Synchronous API, and more
            </a>
          </h5>
          <div class="content has-text-justified">
          <div class="authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
            <a href="mailto:hello@lmql.ai">LMQL Team</a>
        </span>
            </div>
          </div>
            <div class="date">
                Thu, Jun 8, 2023
            </div>
            <section id="releasing-lmql-0-0-6-4-lmtp-azure-synchronous-api-and-more">

<p>Among many things, this update contains several bug fixes and improvements. The most notable changes are:</p>
<ul>
<li><p><strong>Azure OpenAI support</strong> LMQL now supports OpenAI models that are served via Azure. For more information on how to use Azure models, please see the corresponding chapter in the <a class="reference external" href="https://docs.lmql.ai/en/stable/language/azure.html">documentation</a>. Many thanks to <a class="reference external" href="https://github.com/veqtor">@veqtor</a> for contributing this feature.</p></li>
<li><p><strong>Local Models via the Language Model Transport Protocol</strong> LMQL 0.0.6.4 implements a novel protocol to stream token output from local models, vastly improving performance. In our first benchmarks, we observed a 5-6x speedup for local model inference. For more information on how to use local models, please see the corresponding chapter in the <a class="reference external" href="https://docs.lmql.ai/en/stable/language/hf.html">documentation</a>.</p>
<p>To learn more about the internals of the new streaming protocol, i.e. the language model transport protocol (LMTP), you can find more details in <a class="reference external" href="https://github.com/eth-sri/lmql/blob/main/src/lmql/models/lmtp/README.md">this README file</a>. In the future, we intend to implement more model backends using LMTP, streamlining communication between LMQL and models.</p>
 <div style="text-align:center">
  <img src="https://docs.lmql.ai/en/stable/_images/inference.svg" width="80%">
  <br>
  <i>LMQL's new streaming protocol (LMTP) allows for faster local model inference.</i>
 </div>
</li>
<li><p><strong>Synchronous Python API</strong> Next to an <code class="docutils literal notranslate"><span class="pre">async/await</span></code> based API, LMQL now also provides a synchronous API. This means you no longer need to use <code class="docutils literal notranslate"><span class="pre">asyncio</span></code> to use LMQL from Python.</p>
<p>To use the synchronous API, simply declare <code class="docutils literal notranslate"><span class="pre">@lmql.query</span></code> function without the <code class="docutils literal notranslate"><span class="pre">async</span></code> keyword, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">lmql</span>

<p><span class="nd">@lmql</span><span class="o">.</span><span class="n">query</span>
<span class="k">def</span> <span class="nf">hello</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;lmql</span>
<span class="sd">    argmax </span>
<span class="sd">        &quot;Hello {s} [RESPONSE]&quot; </span>
<span class="sd">        return RESPONSE</span>
<span class="sd">    from </span>
<span class="sd">        &quot;chatgpt&quot;</span>
<span class="sd">    &#39;&#39;&#39;</span></p>
<p><span class="nb">print</span><span class="p">(</span><span class="n">hello</span><span class="p">(</span><span class="s2">&quot;world&quot;</span><span class="p">))</span> <span class="c1"># [&#39;Hello! How can I assist you today?&#39;]</span>
</pre></div></p>
</div>
<p>If you instead want to use <code class="docutils literal notranslate"><span class="pre">lmql.run</span></code> in a synchronous context, you can now use <code class="docutils literal notranslate"><span class="pre">lmql.run_sync</span></code> instead. To learn more about how LMQL can be used from Python, check out our <a class="reference external" href="https://docs.lmql.ai/en/stable/python/python.html">documentation</a>.</p>
</li>
<li><p><strong>Improved Tokenizer Backends</strong> LMQL can now use the excellent <a class="reference external" href="https://github.com/openai/tiktoken"><code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> tokenizer</a> as tokenization backend (for OpenAI models). Furthermore, all tokenization backends have been ported to operate on a byte-level, which improves support for multibyte characters and emojis. This is especially relevant for non-English languages and special characters.</p></li>
<li><p><strong>Docker Image</strong> LMQL now provides a Docker image that can be used to run the LMQL playground in a containerized environment. For more information, please see the <a class="reference external" href="https://docs.lmql.ai/en/stable/docker-setup.html">documentation</a>. Many thanks to <a class="reference external" href="https://github.com/SilacciA">@SilacciA</a> for contributing this feature.</p></li>
<li><p><strong>Faster Startup Time</strong> We optimized LMQL‚Äôs import hierarchy, which results in faster module loading time.</p></li>
</ul>
</section>



          </div>
        </div>
    </div>
  </section>
    <a class='blog see-all' href="/blog">See all blog posts</a>

<section class="section custom-footer">
  <div class="container is-max-desktop content">
    <div class="columns">
      <div class="column" style="text-align: left;">
        LMQL is a project by the <a href="https://www.sri.inf.ethz.ch/">Secure, Reliable, and Intelligent Systems Lab</a> at ETH Z√ºrich.<br/>
        <img src="static/images/institution-logos.svg"/>
      </div>
      <div class="column" style="text-align: left;">
        Site template adapted from <a href="http://nerfies.github.io/">Nerfies</a> by Keunhong Park et al. and uses <a href="https://bulma.io/">Bulma</a>.<br/>
        Last updated on Mon, Jul 17, 7:06 PM (UTC)<br/>
      </div>
    </div>
  </div>
</section>

</body>
</html>
